{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import numpy as np\n",
    "from urlextract import URLExtract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making datasets for labeling\n",
    "There are several stages to this, and I'm going to pull samples to label from three sources.\n",
    "\n",
    "First, I'm limiting this to the subset of papers that are funded by the NIMH.\n",
    "\n",
    "Using that subset, I'm going to label 'passages' for whether they contain an instance of data sharing. A passage roughly corresponds to a paragraph in the paper, but sometimes is a footnote or a reference or a title. It's an organizational unit within the PMC fulltext databnase.\n",
    "\n",
    "The sources that I'm pulling these passages from are:\n",
    "\n",
    "1. Contexts previously identified as possible data-sharing using regular expressions.\n",
    "2. A list of papers described as having shared data using NDAR. For this, I'll attempt to use any passages where data sharing is described\n",
    "3. A random selection of passsages that are unlikely to contain data-sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull segments for labeling\n",
    "extractor = URLExtract()\n",
    "\n",
    "def hit_contexts(r_out, include_urls = False):\n",
    "    out_dat = []\n",
    "    \n",
    "    reg_matches = re.compile(r\"\"\"(github)|(osf\\.io)|(nda\\.nih\\.gov)|(openneuro)|(\\sndar)|\n",
    "                                 (national database for autism research)|(brain-map\\.org)|\n",
    "                                 (humanconnectome\\.org)|(balsa\\.wustl\\.edu)|(loni\\.usc\\.edu)|\n",
    "                                 (ida\\.loni\\.usc\\.edu)|(fmridc)|(ccrns)|(datalad)|(dataverse)|\n",
    "                                 (dbgap)|(nih\\.gov\\/gap)|(dryad)|(figshare)|(fcon_1000\\.projects)|\n",
    "                                 (nitrc)|(mcgill\\.ca\\/bic\\/resources\\/omega)|(xnat\\.org)|\n",
    "                                 (zenodo)|(opendata\\.aws)\"\"\", re.X)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        tmp_doi = r_out['documents'][0]['passages'][0]['infons']['article-id_doi']\n",
    "    except:\n",
    "        tmp_doi = None\n",
    "    try:\n",
    "        tmp_pmcid = r_out['documents'][0]['id']\n",
    "    except:\n",
    "        tmp_pmcid = None\n",
    "        \n",
    "    for passage in r_out['documents'][0]['passages']:\n",
    "        passage_marked = 0\n",
    "        m = re.finditer(reg_matches, passage['text'].lower())\n",
    "        if m:\n",
    "            segments = [(max(0, item.start()-175),\n",
    "                         min(len(passage['text']), item.end(0)+125),\n",
    "                         item.group()) for item in m]\n",
    "            \n",
    "            try:\n",
    "                section_type = passage['infons']['section_type']\n",
    "            except:\n",
    "                section_type=None\n",
    "            \n",
    "            for seg in segments:\n",
    "                out_dat.append([passage['text'], #delete the seg[0]:seg[1] and you'll have the full segment\n",
    "                                seg[2], #ids which repo\n",
    "                                passage['offset'], #how far into the paper?\n",
    "                                tmp_pmcid, \n",
    "                                tmp_doi, \n",
    "                                section_type])\n",
    "            passage_marked = 1\n",
    "        \n",
    "        if include_urls:\n",
    "            if extractor.has_urls(passage['text'].lower()):\n",
    "                try:\n",
    "                    section_type = passage['infons']['section_type']\n",
    "                except:\n",
    "                    section_type=None\n",
    "\n",
    "                out_dat.append([passage['text'],\n",
    "                                'url_hit',\n",
    "                                passage['offset'],\n",
    "                                tmp_pmcid,\n",
    "                                tmp_doi,\n",
    "                                section_type])\n",
    "                passage_marked = 1\n",
    "            \n",
    "        if passage_marked == 0:\n",
    "            out_dat.append([None, None, None, tmp_pmcid, tmp_doi, None])\n",
    "        \n",
    "    return(out_dat)\n",
    "\n",
    "def sample_section(paper):\n",
    "    candidate = []\n",
    "    for i in paper['documents'][0]['passages']:\n",
    "        if i['infons']['section_type'] != 'REF':\n",
    "            candidate.append(i)\n",
    "    \n",
    "    selection = random.choice(candidate)\n",
    "    \n",
    "    return(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts previously identified as probable data sharing (via regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load previously identified hits and papers funded by nimh\n",
    "hits = pd.read_csv('output/hit_contexts.csv') #need the subset that is in nimh funded papers\n",
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "#load file index\n",
    "file_ix = pd.read_csv('output/file_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5726, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter the hits to just nimh papers\n",
    "nimh_hits = hits[hits.pmcid.isin(nimh_papers.pmcid)]\n",
    "nimh_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(nimh_hits.pmcid.unique()).sample(100, replace=False)\n",
    "sampled_papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get location of sampled hits\n",
    "file_locs = file_ix[file_ix.pmcid.isin(sampled_papers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_collect = []\n",
    "for i, file_path in enumerate(file_locs.file):\n",
    "    with open(file_path[24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[file_locs.paper_number.iloc[i]]\n",
    "        out = hit_contexts(paper)\n",
    "        data_collect.extend(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_label = pd.DataFrame(data_collect, columns=['text', 'repo', 'paper_offset', 'pmcid', 'doi', 'section'])\n",
    "df_to_label = df_to_label.drop_duplicates(subset=['pmcid'])\n",
    "df_to_label.to_csv('output/labeled_data/regex_hits.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers identified through NDAR\n",
    "\n",
    "This is a convoluted process. First, I pull out all the papers mentioned in scraped NDA collections as being relevant. Next, I got a sample of those papers, and manually labeled them for containing instances of data sharing. With those labeled papers, I pulled out the section of text in each paper that mentions data sharing. If there was no such mention, then I just pulled out a random section (leaving out anything that was a reference). I then put these data together and wrote them as a csv file (`ndar_labs.csv`). Though I aimed for a sample of 100, I ended up with 65 due to a few papers being embargoed, and some papers listed in NDAR not being in the full-text database that I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_collections = glob.glob('output/ndar_collections/*')\n",
    "ndar_collections.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ndar_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for collection in ndar_collections:\n",
    "    soup = BeautifulSoup(open(collection), \"html.parser\")\n",
    "    table = soup.find_all(id='publication-table')[0]\n",
    "    table_rows = table.find_all('tr')\n",
    "    for tr in table_rows:\n",
    "        td = tr.find_all('td')\n",
    "        row = [tr.text for tr in td]\n",
    "        l.append(row)\n",
    "        \n",
    "collection_pubs = pd.DataFrame(l, columns = ['pmid', 'study', 'title', 'journal', 'authors', 'date', 'status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick out the pmids for the studies marked as relevant\n",
    "collection_pmids = pd.Series(collection_pubs.pmid[collection_pubs.status=='Relevant'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (4,5,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read in the linking file and convert pmids to integer\n",
    "pmid_to_pmcid = pd.read_csv('data/PMC-ids.csv')\n",
    "pmid_to_pmcid['pmid'] = pmid_to_pmcid.PMID.fillna(0.0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal Title</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>eISSN</th>\n",
       "      <th>Year</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Page</th>\n",
       "      <th>DOI</th>\n",
       "      <th>PMCID</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Manuscript Id</th>\n",
       "      <th>Release Date</th>\n",
       "      <th>pmid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC13900</td>\n",
       "      <td>11250746.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11250746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC13901</td>\n",
       "      <td>11250747.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11250747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC13902</td>\n",
       "      <td>11250748.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11250748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>1999</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>10.1186/bcr29</td>\n",
       "      <td>PMC13911</td>\n",
       "      <td>11056684.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11056684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>1999</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC13912</td>\n",
       "      <td>11400682.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11400682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Journal Title       ISSN      eISSN  Year Volume Issue Page  \\\n",
       "0  Breast Cancer Res  1465-5411  1465-542X  2000      3     1   55   \n",
       "1  Breast Cancer Res  1465-5411  1465-542X  2000      3     1   61   \n",
       "2  Breast Cancer Res  1465-5411  1465-542X  2000      3     1   66   \n",
       "3  Breast Cancer Res  1465-5411  1465-542X  1999      2     1   59   \n",
       "4  Breast Cancer Res  1465-5411  1465-542X  1999      2     1   64   \n",
       "\n",
       "             DOI     PMCID        PMID Manuscript Id Release Date      pmid  \n",
       "0            NaN  PMC13900  11250746.0           NaN         live  11250746  \n",
       "1            NaN  PMC13901  11250747.0           NaN         live  11250747  \n",
       "2            NaN  PMC13902  11250748.0           NaN         live  11250748  \n",
       "3  10.1186/bcr29  PMC13911  11056684.0           NaN         live  11056684  \n",
       "4            NaN  PMC13912  11400682.0           NaN         live  11400682  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmid_to_pmcid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#keep the rows in the linking file that have a match from ndar; strip out letters from pmcid\n",
    "referenced_papers = pmid_to_pmcid[pmid_to_pmcid.pmid.isin(collection_pmids)]\n",
    "referenced_papers['pmcid'] = referenced_papers.PMCID.apply(lambda x: x[3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(referenced_papers.pmcid.unique()).sample(100, replace=False)\n",
    "#this file was manually labeled so that I can get specific passages\n",
    "sampled_papers.to_csv('output/ndar_pmcids.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_papers = pd.read_csv('output/ndar_pmcids.csv')\n",
    "#split out those that have shared and not shared data\n",
    "no_shares = sampled_papers[sampled_papers.data_sharing=='0']\n",
    "shares = sampled_papers[sampled_papers.data_sharing=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shares_locs = file_ix[file_ix.pmcid.isin(no_shares.PMCID.tolist())]\n",
    "text = []\n",
    "repo = []\n",
    "paper_offset = []\n",
    "pmcid = []\n",
    "doi = []\n",
    "section_type = []\n",
    "for i, file_path in enumerate(no_shares_locs.file):\n",
    "    with open(no_shares_locs.file.iloc[i][24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[no_shares_locs.paper_number.iloc[i]]\n",
    "        section = sample_section(paper)\n",
    "        text.append(section['text'])\n",
    "        repo.append(np.nan)\n",
    "        paper_offset.append(section['offset'])\n",
    "        pmcid.append(no_shares_locs.pmcid.iloc[i])\n",
    "        doi.append(np.nan)\n",
    "        section_type.append(section['infons']['section_type'])\n",
    "        \n",
    "dat = pd.DataFrame({'text':text,\n",
    "                    'repo':repo,\n",
    "                    'paper_offset':paper_offset,\n",
    "                    'pmcid':pmcid,\n",
    "                    'doi':doi,\n",
    "                    'section':section_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares_locs = file_ix[file_ix.pmcid.isin(shares.PMCID.tolist())]\n",
    "text = []\n",
    "repo = []\n",
    "paper_offset = []\n",
    "pmcid = []\n",
    "doi = []\n",
    "section_type = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[0][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[0]]\n",
    "    text.append(paper['documents'][0]['passages'][77]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][77]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[0])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][77]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[2][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[2]]\n",
    "    text.append(paper['documents'][0]['passages'][62]['text'])\n",
    "    repo.append('GEO')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][62]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[2])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][62]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[3][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[3]]\n",
    "    text.append(paper['documents'][0]['passages'][88]['text'])\n",
    "    repo.append('OSF')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][88]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[3])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][88]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[4][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[4]]\n",
    "    text.append(paper['documents'][0]['passages'][47]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][47]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[4])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][47]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[5][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[5]]\n",
    "    text.append(paper['documents'][0]['passages'][91]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][91]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[5])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][91]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2 = pd.DataFrame({'text':text,\n",
    "                    'repo':repo,\n",
    "                    'paper_offset':paper_offset,\n",
    "                    'pmcid':pmcid,\n",
    "                    'doi':doi,\n",
    "                    'section':section_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_labs = pd.concat([dat, dat2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_labs.to_csv('output/labeled_data/ndar_labs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do a random selection of other passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "#load file index\n",
    "file_ix = pd.read_csv('output/file_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(nimh_papers.pmcid.unique()).sample(100, replace=False)\n",
    "sampled_papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 3)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_locs = file_ix[file_ix.pmcid.isin(sampled_papers)]\n",
    "file_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dat = []\n",
    "for i, file_path in enumerate(file_locs.file):\n",
    "    with open(file_path[24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[file_locs.paper_number.iloc[i]]\n",
    "        sec = sample_section(paper)\n",
    "        out_dat.append([sec['text'], #delete the seg[0]:seg[1] and you'll have the full segment\n",
    "                np.nan, #ids which repo\n",
    "                sec['offset'], #how far into the paper?\n",
    "                file_locs.pmcid.iloc[i], \n",
    "                np.nan, \n",
    "                sec['infons']['section_type']])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(out_dat, columns=['text', 'repo', 'paper_offset', 'pmcid', 'doi', 'section'])\n",
    "temp.to_csv('output/labeled_data/random_selections.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to add some additional cases\n",
    "\n",
    "First, I'm going to pull out some of the instances identified via regex matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5726, 5)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load previously identified hits and papers funded by nimh\n",
    "hits = pd.read_csv('output/hit_contexts.csv') #need the subset that is in nimh funded papers\n",
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "#load file index\n",
    "file_ix = pd.read_csv('output/file_index.csv')\n",
    "#filter the hits to just nimh papers\n",
    "nimh_hits = hits[hits.pmcid.isin(nimh_papers.pmcid)]\n",
    "nimh_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5463, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out the previously sampled cases\n",
    "excludes = pd.read_csv('output/labeled_data/regex_hits.csv')\n",
    "nimh_hits = nimh_hits[~nimh_hits.pmcid.isin(excludes.pmcid)]\n",
    "print(nimh_hits.shape)\n",
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(nimh_hits.pmcid.unique()).sample(100, replace=False)\n",
    "sampled_papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get location of sampled hits\n",
    "file_locs = file_ix[file_ix.pmcid.isin(sampled_papers)]\n",
    "data_collect = []\n",
    "for i, file_path in enumerate(file_locs.file):\n",
    "    with open(file_path[24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[file_locs.paper_number.iloc[i]]\n",
    "        out = hit_contexts(paper)\n",
    "        data_collect.extend(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_label = pd.DataFrame(data_collect, columns=['text', 'repo', 'paper_offset', 'pmcid', 'doi', 'section'])\n",
    "df_to_label = df_to_label.drop_duplicates(subset=['pmcid'])\n",
    "df_to_label.to_csv('output/labeled_data/regex_hits2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional cases - NDAR selections2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_collections = glob.glob('output/ndar_collections/*')\n",
    "ndar_collections.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for collection in ndar_collections:\n",
    "    soup = BeautifulSoup(open(collection), \"html.parser\")\n",
    "    table = soup.find_all(id='publication-table')[0]\n",
    "    table_rows = table.find_all('tr')\n",
    "    for tr in table_rows:\n",
    "        td = tr.find_all('td')\n",
    "        row = [tr.text for tr in td]\n",
    "        l.append(row)\n",
    "        \n",
    "collection_pubs = pd.DataFrame(l, columns = ['pmid', 'study', 'title', 'journal', 'authors', 'date', 'status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riddleta/ac_knowl/ac_knowl/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (4,5,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/riddleta/ac_knowl/ac_knowl/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#pick out the pmids for the studies marked as relevant\n",
    "collection_pmids = pd.Series(collection_pubs.pmid[collection_pubs.status=='Relevant'].unique())\n",
    "\n",
    "#read in the linking file and convert pmids to integer\n",
    "pmid_to_pmcid = pd.read_csv('data/PMC-ids.csv')\n",
    "pmid_to_pmcid['pmid'] = pmid_to_pmcid.PMID.fillna(0.0).astype(int)\n",
    "\n",
    "#keep the rows in the linking file that have a match from ndar; strip out letters from pmcid\n",
    "referenced_papers = pmid_to_pmcid[pmid_to_pmcid.pmid.isin(collection_pmids)]\n",
    "referenced_papers['pmcid'] = referenced_papers.PMCID.apply(lambda x: x[3:])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to just nimh funded work\n",
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "nimh_pmcids = nimh_papers.pmcid.astype('str').tolist()\n",
    "\n",
    "referenced_papers = referenced_papers[referenced_papers.pmcid.isin(nimh_pmcids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 14)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter out the already-labeled ndar data\n",
    "df_ndar_labs = pd.read_csv('output/labeled_data/ndar_labs.csv')\n",
    "labeled_ndars = df_ndar_labs.pmcid.astype('str').tolist()\n",
    "referenced_papers = referenced_papers[~referenced_papers.pmcid.isin(labeled_ndars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riddleta/ac_knowl/ac_knowl/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#get the file location for the matches\n",
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(referenced_papers.pmcid.unique()).sample(100, replace=False)\n",
    "\n",
    "#this file was manually labeled so that I can get specific passages\n",
    "sampled_papers.to_csv('output/ndar_pmcids2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riddleta/ac_knowl/ac_knowl/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "sampled_papers = pd.read_csv('output/ndar_pmcids2.csv')\n",
    "file_ix = pd.read_csv('output/file_index.csv')\n",
    "#split out those that have shared and not shared data\n",
    "no_shares = sampled_papers[sampled_papers.open_data==0]\n",
    "shares = sampled_papers[sampled_papers.open_data==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shares_locs = file_ix[file_ix.pmcid.astype('str').isin(no_shares.PMCID.astype('str').tolist())]\n",
    "text = []\n",
    "repo = []\n",
    "paper_offset = []\n",
    "pmcid = []\n",
    "doi = []\n",
    "section_type = []\n",
    "for i, file_path in enumerate(no_shares_locs.file):\n",
    "    with open(no_shares_locs.file.iloc[i][24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[no_shares_locs.paper_number.iloc[i]]\n",
    "        section = sample_section(paper)\n",
    "        text.append(section['text'])\n",
    "        repo.append(np.nan)\n",
    "        paper_offset.append(section['offset'])\n",
    "        pmcid.append(no_shares_locs.pmcid.iloc[i])\n",
    "        doi.append(np.nan)\n",
    "        section_type.append(section['infons']['section_type'])\n",
    "        \n",
    "dat = pd.DataFrame({'text':text,\n",
    "                    'repo':repo,\n",
    "                    'paper_offset':paper_offset,\n",
    "                    'pmcid':pmcid,\n",
    "                    'doi':doi,\n",
    "                    'section':section_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares_locs = file_ix[file_ix.pmcid.astype('str').isin(shares.PMCID.astype('str').tolist())]\n",
    "text = []\n",
    "repo = []\n",
    "paper_offset = []\n",
    "pmcid = []\n",
    "doi = []\n",
    "section_type = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][61]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][61]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][61]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][16]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][16]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][16]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][50]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][50]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][50]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][15]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][15]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][15]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][42]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][42]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][42]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][11]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][11]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][11]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][49]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][49]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][49]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][83]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][83]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][83]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][26]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][26]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][26]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][43]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][43]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][43]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][11]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][11]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][11]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 11\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][44]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][44]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][44]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 12\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][60]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][60]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][60]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 13\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][54]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][54]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][54]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 14\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][45]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][45]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][45]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][11]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][11]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][11]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 16\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][54]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][54]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][54]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 17\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][51]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][51]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][51]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2 = pd.DataFrame({'text':text,\n",
    "                    'repo':repo,\n",
    "                    'paper_offset':paper_offset,\n",
    "                    'pmcid':pmcid,\n",
    "                    'doi':doi,\n",
    "                    'section':section_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_labs = pd.concat([dat, dat2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_papers['pmcid'] = sampled_papers.PMCID.astype('str')\n",
    "ndar_labs['pmcid'] = ndar_labs.pmcid.astype('str')\n",
    "ndar_labs = ndar_labs.merge(sampled_papers[['pmcid', 'data_sharing', 'open_data']], how='left', on='pmcid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_labs.to_csv('output/labeled_data/ndar_labs2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all nimh passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riddleta/ac_knowl/ac_knowl/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "#load file index\n",
    "file_ix = pd.read_csv('output/file_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ix['pmcid'] = file_ix.pmcid.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimh_papers['pmcid'] = nimh_papers.pmcid.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57692, 3)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_papers = file_ix[file_ix.pmcid.isin(nimh_papers.pmcid)]\n",
    "target_papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_papers = target_papers.sort_values('file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>paper_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102548</th>\n",
       "      <td>/home/riddleta/ac_knowl/output/full_texts/pape...</td>\n",
       "      <td>4388653</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104899</th>\n",
       "      <td>/home/riddleta/ac_knowl/output/full_texts/pape...</td>\n",
       "      <td>4392339</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104875</th>\n",
       "      <td>/home/riddleta/ac_knowl/output/full_texts/pape...</td>\n",
       "      <td>4392315</td>\n",
       "      <td>2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104785</th>\n",
       "      <td>/home/riddleta/ac_knowl/output/full_texts/pape...</td>\n",
       "      <td>4392168</td>\n",
       "      <td>2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104484</th>\n",
       "      <td>/home/riddleta/ac_knowl/output/full_texts/pape...</td>\n",
       "      <td>4391730</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     file    pmcid  \\\n",
       "102548  /home/riddleta/ac_knowl/output/full_texts/pape...  4388653   \n",
       "104899  /home/riddleta/ac_knowl/output/full_texts/pape...  4392339   \n",
       "104875  /home/riddleta/ac_knowl/output/full_texts/pape...  4392315   \n",
       "104785  /home/riddleta/ac_knowl/output/full_texts/pape...  4392168   \n",
       "104484  /home/riddleta/ac_knowl/output/full_texts/pape...  4391730   \n",
       "\n",
       "        paper_number  \n",
       "102548            49  \n",
       "104899          2400  \n",
       "104875          2376  \n",
       "104785          2286  \n",
       "104484          1985  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_prints = range(0, len(target_papers.file.tolist()), 250)\n",
    "len(status_prints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "250\n",
      "500\n",
      "750\n",
      "1000\n",
      "1250\n",
      "1500\n",
      "1750\n",
      "2000\n",
      "2250\n",
      "2500\n",
      "2750\n",
      "3000\n",
      "3250\n",
      "3500\n",
      "3750\n",
      "4000\n",
      "4250\n",
      "4500\n",
      "4750\n",
      "5000\n",
      "5250\n",
      "5500\n",
      "5750\n",
      "6000\n",
      "6250\n",
      "6500\n",
      "6750\n",
      "7000\n",
      "7250\n",
      "7500\n",
      "7750\n",
      "8000\n",
      "8250\n",
      "8500\n",
      "8750\n",
      "9000\n",
      "9250\n",
      "9500\n",
      "9750\n",
      "10000\n",
      "10250\n",
      "10500\n",
      "10750\n",
      "11000\n",
      "11250\n",
      "11500\n",
      "11750\n",
      "12000\n",
      "12250\n",
      "12500\n",
      "12750\n",
      "13000\n",
      "13250\n",
      "13500\n",
      "13750\n",
      "14000\n",
      "14250\n",
      "14500\n",
      "14750\n",
      "15000\n",
      "15250\n",
      "15500\n",
      "15750\n",
      "16000\n",
      "16250\n",
      "16500\n",
      "16750\n",
      "17000\n",
      "17250\n",
      "17500\n",
      "17750\n",
      "18000\n",
      "18250\n",
      "18500\n",
      "18750\n",
      "19000\n",
      "19250\n",
      "19500\n",
      "19750\n",
      "20000\n",
      "20250\n",
      "20500\n",
      "20750\n",
      "21000\n",
      "21250\n",
      "21500\n",
      "21750\n",
      "22000\n",
      "22250\n",
      "22500\n",
      "22750\n",
      "23000\n",
      "23250\n",
      "23500\n",
      "23750\n",
      "24000\n",
      "24250\n",
      "24500\n",
      "24750\n",
      "25000\n",
      "25250\n",
      "25500\n",
      "25750\n",
      "26000\n",
      "26250\n",
      "26500\n",
      "26750\n",
      "27000\n",
      "27250\n",
      "27500\n",
      "27750\n",
      "28000\n",
      "28250\n",
      "28500\n",
      "28750\n",
      "29000\n",
      "29250\n",
      "29500\n",
      "29750\n",
      "30000\n",
      "30250\n",
      "30500\n",
      "30750\n",
      "31000\n",
      "31250\n",
      "31500\n",
      "31750\n",
      "32000\n",
      "32250\n",
      "32500\n",
      "32750\n",
      "33000\n",
      "33250\n",
      "33500\n",
      "33750\n",
      "34000\n",
      "34250\n",
      "34500\n",
      "34750\n",
      "35000\n",
      "35250\n",
      "35500\n",
      "35750\n",
      "36000\n",
      "36250\n",
      "36500\n",
      "36750\n",
      "37000\n",
      "37250\n",
      "37500\n",
      "37750\n",
      "38000\n",
      "38250\n",
      "38500\n",
      "38750\n",
      "39000\n",
      "39250\n",
      "39500\n",
      "39750\n",
      "40000\n",
      "40250\n",
      "40500\n",
      "40750\n",
      "41000\n",
      "41250\n",
      "41500\n",
      "41750\n",
      "42000\n",
      "42250\n",
      "42500\n",
      "42750\n",
      "43000\n",
      "43250\n",
      "43500\n",
      "43750\n",
      "44000\n",
      "44250\n",
      "44500\n",
      "44750\n",
      "45000\n",
      "45250\n",
      "45500\n",
      "45750\n",
      "46000\n",
      "46250\n",
      "46500\n",
      "46750\n",
      "47000\n",
      "47250\n",
      "47500\n",
      "47750\n",
      "48000\n",
      "48250\n",
      "48500\n",
      "48750\n",
      "49000\n",
      "49250\n",
      "49500\n",
      "49750\n",
      "50000\n",
      "50250\n",
      "50500\n",
      "50750\n",
      "51000\n",
      "51250\n",
      "51500\n",
      "51750\n",
      "52000\n",
      "52250\n",
      "52500\n",
      "52750\n",
      "53000\n",
      "53250\n",
      "53500\n",
      "53750\n",
      "54000\n",
      "54250\n",
      "54500\n",
      "54750\n",
      "55000\n",
      "55250\n",
      "55500\n",
      "55750\n",
      "56000\n",
      "56250\n",
      "56500\n",
      "56750\n",
      "57000\n",
      "57250\n",
      "57500\n"
     ]
    }
   ],
   "source": [
    "data_collect = []\n",
    "last_file = np.nan\n",
    "for i, file in enumerate(target_papers.file.tolist()):\n",
    "    if i in status_prints:\n",
    "        print(i)\n",
    "    if file == last_file:\n",
    "        paper = dat[target_papers.paper_number.iloc[i]]\n",
    "        out_dat = hit_contexts(paper, include_urls=True)\n",
    "        data_collect.extend(out_dat)\n",
    "    else:\n",
    "        with open(file) as infile:\n",
    "            dat = json.load(infile)\n",
    "            paper = dat[target_papers.paper_number.iloc[i]]\n",
    "            out_dat = hit_contexts(paper, include_urls=True)\n",
    "            data_collect.extend(out_dat)\n",
    "            last_file = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49376"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>repo_hit</th>\n",
       "      <th>paper_offset</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>doi</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>While the number of simultaneously-recorded ne...</td>\n",
       "      <td>url_hit</td>\n",
       "      <td>1663</td>\n",
       "      <td>4392339</td>\n",
       "      <td>10.1016/j.neuron.2015.01.028</td>\n",
       "      <td>INTRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recent rapid progress with on-head digital mul...</td>\n",
       "      <td>github</td>\n",
       "      <td>23918</td>\n",
       "      <td>4392339</td>\n",
       "      <td>10.1016/j.neuron.2015.01.028</td>\n",
       "      <td>INTRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recent rapid progress with on-head digital mul...</td>\n",
       "      <td>url_hit</td>\n",
       "      <td>23918</td>\n",
       "      <td>4392339</td>\n",
       "      <td>10.1016/j.neuron.2015.01.028</td>\n",
       "      <td>INTRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spectacular progress has been made over the pa...</td>\n",
       "      <td>url_hit</td>\n",
       "      <td>36421</td>\n",
       "      <td>4392339</td>\n",
       "      <td>10.1016/j.neuron.2015.01.028</td>\n",
       "      <td>CONCL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obtaining high-quality, high-density data from...</td>\n",
       "      <td>url_hit</td>\n",
       "      <td>37891</td>\n",
       "      <td>4392339</td>\n",
       "      <td>10.1016/j.neuron.2015.01.028</td>\n",
       "      <td>CONCL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context repo_hit  paper_offset  \\\n",
       "0  While the number of simultaneously-recorded ne...  url_hit          1663   \n",
       "1  Recent rapid progress with on-head digital mul...   github         23918   \n",
       "2  Recent rapid progress with on-head digital mul...  url_hit         23918   \n",
       "3  Spectacular progress has been made over the pa...  url_hit         36421   \n",
       "4  Obtaining high-quality, high-density data from...  url_hit         37891   \n",
       "\n",
       "     pmcid                           doi section  \n",
       "0  4392339  10.1016/j.neuron.2015.01.028   INTRO  \n",
       "1  4392339  10.1016/j.neuron.2015.01.028   INTRO  \n",
       "2  4392339  10.1016/j.neuron.2015.01.028   INTRO  \n",
       "3  4392339  10.1016/j.neuron.2015.01.028   CONCL  \n",
       "4  4392339  10.1016/j.neuron.2015.01.028   CONCL  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data_collect)\n",
    "df.columns = ['context', 'repo_hit', 'paper_offset', 'pmcid', 'doi', 'section']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output/hit_contexts_nimh_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dat['pmcid_url'] = out_dat.pmcid.apply(lambda x: 'http://ncbi.nlm.nih.gov/pmc/articles/PMC'+str(x))\n",
    "out_dat['doi_url'] = out_dat.doi.apply(lambda x: 'http://doi.org/'+str(x))\n",
    "out_dat.to_excel('output/hit_contexts.nimh.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18449"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_dat.pmcid.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull pubs out of HCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open('data/HCP_Publications.htm'), \"html.parser\")\n",
    "pubs = soup.find_all('div', {\"class\": \"publication-data-wrapper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in pubs:\n",
    "    title = i.find('h4').text.strip()\n",
    "    auths = i.find('div', {'class':'publication-authors'}).text.strip()\n",
    "    journal = i.find('span', {'class':'publication-data-name'}).text.strip()\n",
    "    date = i.find('span', {'class':'publication-data-date'}).text.strip()\n",
    "    try:\n",
    "        pmid = i.find('span', {'class':'publication-data-pmid'}).text.strip()[6:]\n",
    "    except:\n",
    "        pmid = np.nan\n",
    "    try:\n",
    "        doi_href = i.find('span', {'class':'publication-data-name'}).find('a')['href']\n",
    "    except:\n",
    "        doi_href = np.nan\n",
    "    l.append([title, auths, journal, date, pmid, doi_href])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(l, columns = ['title', 'auths', 'journal', 'date', 'pmid', 'doi_href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output/hcp_pubs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out some passages identified via regex (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5726, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load previously identified hits and papers funded by nimh\n",
    "hits = pd.read_csv('output/hit_contexts.csv') #need the subset that is in nimh funded papers\n",
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "#load file index\n",
    "file_ix = pd.read_csv('output/file_index.csv')\n",
    "#filter the hits to just nimh papers\n",
    "nimh_hits = hits[hits.pmcid.isin(nimh_papers.pmcid)]\n",
    "nimh_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5470, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out the previously sampled cases\n",
    "excludes = pd.read_csv('output/labeled_data/regex_hits.csv')\n",
    "nimh_hits = nimh_hits[~nimh_hits.pmcid.isin(excludes.pmcid)]\n",
    "print(nimh_hits.shape)\n",
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(nimh_hits.pmcid.unique()).sample(100, replace=False)\n",
    "sampled_papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get location of sampled hits\n",
    "file_locs = file_ix[file_ix.pmcid.isin(sampled_papers)]\n",
    "data_collect = []\n",
    "for i, file_path in enumerate(file_locs.file):\n",
    "    with open(file_path[24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[file_locs.paper_number.iloc[i]]\n",
    "        out = hit_contexts(paper)\n",
    "        data_collect.extend(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_label = pd.DataFrame(data_collect, columns=['text', 'repo', 'paper_offset', 'pmcid', 'doi', 'section'])\n",
    "df_to_label = df_to_label.drop_duplicates(subset=['pmcid'])\n",
    "#df_to_label.to_csv('output/labeled_data/regex_hits3.csv', index=False)\n",
    "df_to_label.to_csv('output/labeled_data/regex_hits4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>repo</th>\n",
       "      <th>paper_offset</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>doi</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MRI data from the Human Connectome Project (ht...</td>\n",
       "      <td>humanconnectome.org</td>\n",
       "      <td>16764</td>\n",
       "      <td>5837394</td>\n",
       "      <td>10.1093/brain/awx309</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Variability in single-subject whole-brain func...</td>\n",
       "      <td>nitrc</td>\n",
       "      <td>19580</td>\n",
       "      <td>4146649</td>\n",
       "      <td>10.1038/nn.3778</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genotype calls and hybridization intensity dat...</td>\n",
       "      <td>github</td>\n",
       "      <td>31722</td>\n",
       "      <td>4751547</td>\n",
       "      <td>10.1534/g3.115.022087</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gene expression was measured by RNAseq and qua...</td>\n",
       "      <td>github</td>\n",
       "      <td>8101</td>\n",
       "      <td>5561488</td>\n",
       "      <td>10.1002/ajmg.a.38327</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The GTEx genotype and gene expression data wer...</td>\n",
       "      <td>dbgap</td>\n",
       "      <td>40358</td>\n",
       "      <td>4609956</td>\n",
       "      <td>10.1038/srep15145</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                 repo  \\\n",
       "0  MRI data from the Human Connectome Project (ht...  humanconnectome.org   \n",
       "1  Variability in single-subject whole-brain func...                nitrc   \n",
       "2  Genotype calls and hybridization intensity dat...               github   \n",
       "4  Gene expression was measured by RNAseq and qua...               github   \n",
       "5  The GTEx genotype and gene expression data wer...                dbgap   \n",
       "\n",
       "   paper_offset    pmcid                    doi  section  \n",
       "0         16764  5837394   10.1093/brain/awx309  METHODS  \n",
       "1         19580  4146649        10.1038/nn.3778  METHODS  \n",
       "2         31722  4751547  10.1534/g3.115.022087  METHODS  \n",
       "4          8101  5561488   10.1002/ajmg.a.38327  METHODS  \n",
       "5         40358  4609956      10.1038/srep15145  METHODS  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional cases - NDAR selections3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_collections = glob.glob('output/ndar_collections/*')\n",
    "ndar_collections.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for collection in ndar_collections:\n",
    "    soup = BeautifulSoup(open(collection), \"html.parser\")\n",
    "    table = soup.find_all(id='publication-table')[0]\n",
    "    table_rows = table.find_all('tr')\n",
    "    for tr in table_rows:\n",
    "        td = tr.find_all('td')\n",
    "        row = [tr.text for tr in td]\n",
    "        l.append(row)\n",
    "        \n",
    "collection_pubs = pd.DataFrame(l, columns = ['pmid', 'study', 'title', 'journal', 'authors', 'date', 'status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riddleta/ac_knowl/ac_knowl/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (4,5,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/riddleta/ac_knowl/ac_knowl/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#pick out the pmids for the studies marked as relevant\n",
    "collection_pmids = pd.Series(collection_pubs.pmid[collection_pubs.status=='Relevant'].unique())\n",
    "\n",
    "#read in the linking file and convert pmids to integer\n",
    "pmid_to_pmcid = pd.read_csv('data/PMC-ids.csv')\n",
    "pmid_to_pmcid['pmid'] = pmid_to_pmcid.PMID.fillna(0.0).astype(int)\n",
    "\n",
    "#keep the rows in the linking file that have a match from ndar; strip out letters from pmcid\n",
    "referenced_papers = pmid_to_pmcid[pmid_to_pmcid.pmid.isin(collection_pmids)]\n",
    "referenced_papers['pmcid'] = referenced_papers.PMCID.apply(lambda x: x[3:])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to just nimh funded work\n",
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "nimh_pmcids = nimh_papers.pmcid.astype('str').tolist()\n",
    "\n",
    "referenced_papers = referenced_papers[referenced_papers.pmcid.isin(nimh_pmcids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out the already-labeled ndar data\n",
    "ndar_lab_files = glob.glob('output/labeled_data/ndar*.csv')\n",
    "df_ndar_labs = pd.concat((pd.read_csv(f) for f in ndar_lab_files)).reset_index()\n",
    "labeled_ndars = df_ndar_labs.pmcid.astype('str').tolist()\n",
    "referenced_papers = referenced_papers[~referenced_papers.pmcid.isin(labeled_ndars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riddleta/ac_knowl/ac_knowl/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#get the file location for the matches\n",
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(referenced_papers.pmcid.unique()).sample(100, replace=False)\n",
    "\n",
    "#this file was manually labeled so that I can get specific passages\n",
    "sampled_papers.to_csv('output/ndar_pmcids3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_papers = pd.read_csv('output/ndar_pmcids3.csv')\n",
    "sampled_papers['pmcid'] = sampled_papers.PMCID.astype('int').astype('str')\n",
    "file_ix = pd.read_csv('output/file_index.csv')\n",
    "#split out those that have shared and not shared data\n",
    "no_shares = sampled_papers[sampled_papers.open_data==0]\n",
    "shares = sampled_papers[sampled_papers.open_data==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shares_locs = file_ix[file_ix.pmcid.astype('str').isin(no_shares.PMCID.astype('int').astype('str').tolist())]\n",
    "text = []\n",
    "repo = []\n",
    "paper_offset = []\n",
    "pmcid = []\n",
    "doi = []\n",
    "section_type = []\n",
    "for i, file_path in enumerate(no_shares_locs.file):\n",
    "    with open(no_shares_locs.file.iloc[i][24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[no_shares_locs.paper_number.iloc[i]]\n",
    "        section = sample_section(paper)\n",
    "        text.append(section['text'])\n",
    "        repo.append(np.nan)\n",
    "        paper_offset.append(section['offset'])\n",
    "        pmcid.append(no_shares_locs.pmcid.iloc[i])\n",
    "        doi.append(np.nan)\n",
    "        section_type.append(section['infons']['section_type'])\n",
    "        \n",
    "dat = pd.DataFrame({'text':text,\n",
    "                    'repo':repo,\n",
    "                    'paper_offset':paper_offset,\n",
    "                    'pmcid':pmcid,\n",
    "                    'doi':doi,\n",
    "                    'section':section_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares_locs = file_ix[file_ix.pmcid.astype('str').isin(shares.PMCID.astype('int').astype('str').tolist())]\n",
    "text = []\n",
    "repo = []\n",
    "paper_offset = []\n",
    "pmcid = []\n",
    "doi = []\n",
    "section_type = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shares_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][31]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][31]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][31]['infons']['section_type'])\n",
    "i = 1\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][14]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][14]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][14]['infons']['section_type'])\n",
    "i = 2\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][0]['infons']['notes'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][0]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append('NOTES')\n",
    "i = 3\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][26]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][26]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][26]['infons']['section_type'])\n",
    "i = 4\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][40]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][40]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][40]['infons']['section_type'])\n",
    "i = 5\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][19]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][19]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][19]['infons']['section_type'])\n",
    "i = 6\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][15]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][15]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][15]['infons']['section_type'])\n",
    "i = 7\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][47]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][47]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][47]['infons']['section_type'])\n",
    "i = 8\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][77]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][77]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][77]['infons']['section_type'])\n",
    "i = 9\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][34]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][34]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][34]['infons']['section_type'])\n",
    "i = 10\n",
    "with open(shares_locs.file.iloc[i]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[i]]\n",
    "    text.append(paper['documents'][0]['passages'][84]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][84]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[i])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][84]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2 = pd.DataFrame({'text':text,\n",
    "                    'repo':repo,\n",
    "                    'paper_offset':paper_offset,\n",
    "                    'pmcid':pmcid,\n",
    "                    'doi':doi,\n",
    "                    'section':section_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_labs = pd.concat([dat, dat2])\n",
    "ndar_labs['pmcid'] = ndar_labs.pmcid.astype('str')\n",
    "ndar_labs = ndar_labs.merge(sampled_papers[['pmcid', 'data_sharing', 'open_data']], how='left', on='pmcid')\n",
    "ndar_labs.to_csv('output/labeled_data/ndar_labs3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac_knowl",
   "language": "python",
   "name": "ac_knowl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
