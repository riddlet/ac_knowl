{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making datasets for labeling\n",
    "There are several stages to this, and I'm going to pull samples to label from three sources.\n",
    "\n",
    "First, I'm limiting this to the subset of papers that are funded by the NIMH.\n",
    "\n",
    "Using that subset, I'm going to label 'passages' for whether they contain an instance of data sharing. A passage roughly corresponds to a paragraph in the paper, but sometimes is a footnote or a reference or a title. It's an organizational unit within the PMC fulltext databnase.\n",
    "\n",
    "The sources that I'm pulling these passages from are:\n",
    "\n",
    "1. Contexts previously identified as possible data-sharing using regular expressions.\n",
    "2. A list of papers described as having shared data using NDAR. For this, I'll attempt to use any passages where data sharing is described\n",
    "3. A random selection of passsages that are unlikely to contain data-sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull segments for labeling\n",
    "def hit_contexts(r_out):\n",
    "    out_dat = []\n",
    "    \n",
    "    reg_matches = re.compile(r\"\"\"(github)|(osf\\.io)|(nda\\.nih\\.gov)|(openneuro)|(\\sndar)|\n",
    "                                 (national database for autism research)|(brain-map\\.org)|\n",
    "                                 (humanconnectome\\.org)|(balsa\\.wustl\\.edu)|(loni\\.usc\\.edu)|\n",
    "                                 (ida\\.loni\\.usc\\.edu)|(fmridc)|(ccrns)|(datalad)|(dataverse)|\n",
    "                                 (dbgap)|(nih\\.gov\\/gap)|(dryad)|(figshare)|(fcon_1000\\.projects)|\n",
    "                                 (nitrc)|(mcgill\\.ca\\/bic\\/resources\\/omega)|(xnat\\.org)|\n",
    "                                 (zenodo)|(opendata\\.aws)\"\"\", re.X)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        tmp_doi = r_out['documents'][0]['passages'][0]['infons']['article-id_doi']\n",
    "    except:\n",
    "        tmp_doi = None\n",
    "    try:\n",
    "        tmp_pmcid = r_out['documents'][0]['id']\n",
    "    except:\n",
    "        tmp_pmcid = None\n",
    "        \n",
    "    for passage in r_out['documents'][0]['passages']:\n",
    "        m = re.finditer(reg_matches, passage['text'].lower())\n",
    "        if m:\n",
    "            segments = [(max(0, item.start()-175),\n",
    "                         min(len(passage['text']), item.end(0)+125),\n",
    "                         item.group()) for item in m]\n",
    "            \n",
    "            try:\n",
    "                section_type = passage['infons']['section_type']\n",
    "            except:\n",
    "                section_type=None\n",
    "            \n",
    "            for seg in segments:\n",
    "                out_dat.append([passage['text'], #delete the seg[0]:seg[1] and you'll have the full segment\n",
    "                                seg[2], #ids which repo\n",
    "                                passage['offset'], #how far into the paper?\n",
    "                                tmp_pmcid, \n",
    "                                tmp_doi, \n",
    "                                section_type])\n",
    "        else:\n",
    "            out_dat.append([None, None, tmp_pmcid, tmp_doi, None])\n",
    "        \n",
    "    return(out_dat)\n",
    "\n",
    "def sample_section(paper):\n",
    "    candidate = []\n",
    "    for i in paper['documents'][0]['passages']:\n",
    "        if i['infons']['section_type'] != 'REF':\n",
    "            candidate.append(i)\n",
    "    \n",
    "    selection = random.choice(candidate)\n",
    "    \n",
    "    return(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts previously identified as probable data sharing (via regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load previously identified hits and papers funded by nimh\n",
    "hit_contexts = pd.read_csv('output/hit_contexts.csv') #need the subset that is in nimh funded papers\n",
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "#load file index\n",
    "file_ix = pd.read_csv('output/file_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5726, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter the hits to just nimh papers\n",
    "nimh_hits = hit_contexts[hit_contexts.pmcid.isin(nimh_papers.pmcid)]\n",
    "nimh_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(nimh_hits.pmcid.unique()).sample(100, replace=False)\n",
    "sampled_papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get location of sampled hits\n",
    "file_locs = file_ix[file_ix.pmcid.isin(sampled_papers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_collect = []\n",
    "for i, file_path in enumerate(file_locs.file):\n",
    "    with open(file_path[24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[file_locs.paper_number.iloc[i]]\n",
    "        out = hit_contexts(paper)\n",
    "        data_collect.extend(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_label = pd.DataFrame(data_collect, columns=['text', 'repo', 'paper_offset', 'pmcid', 'doi', 'section'])\n",
    "df_to_label = df_to_label.drop_duplicates(subset=['pmcid'])\n",
    "df_to_label.to_csv('output/labeled_data/regex_hits.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers identified through NDAR\n",
    "\n",
    "This is a convoluted process. First, I pull out all the papers mentioned in scraped NDA collections as being relevant. Next, I got a sample of those papers, and manually labeled them for containing instances of data sharing. With those labeled papers, I pulled out the section of text in each paper that mentions data sharing. If there was no such mention, then I just pulled out a random section (leaving out anything that was a reference). I then put these data together and wrote them as a csv file (`ndar_labs.csv`). Though I aimed for a sample of 100, I ended up with 65 due to a few papers being embargoed, and some papers listed in NDAR not being in the full-text database that I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_collections = glob.glob('output/ndar_collections/*')\n",
    "ndar_collections.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ndar_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for collection in ndar_collections:\n",
    "    soup = BeautifulSoup(open(collection), \"html.parser\")\n",
    "    table = soup.find_all(id='publication-table')[0]\n",
    "    table_rows = table.find_all('tr')\n",
    "    for tr in table_rows:\n",
    "        td = tr.find_all('td')\n",
    "        row = [tr.text for tr in td]\n",
    "        l.append(row)\n",
    "        \n",
    "collection_pubs = pd.DataFrame(l, columns = ['pmid', 'study', 'title', 'journal', 'authors', 'date', 'status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick out the pmids for the studies marked as relevant\n",
    "collection_pmids = pd.Series(collection_pubs.pmid[collection_pubs.status=='Relevant'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (4,5,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read in the linking file and convert pmids to integer\n",
    "pmid_to_pmcid = pd.read_csv('data/PMC-ids.csv')\n",
    "pmid_to_pmcid['pmid'] = pmid_to_pmcid.PMID.fillna(0.0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal Title</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>eISSN</th>\n",
       "      <th>Year</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Page</th>\n",
       "      <th>DOI</th>\n",
       "      <th>PMCID</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Manuscript Id</th>\n",
       "      <th>Release Date</th>\n",
       "      <th>pmid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC13900</td>\n",
       "      <td>11250746.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11250746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC13901</td>\n",
       "      <td>11250747.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11250747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC13902</td>\n",
       "      <td>11250748.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11250748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>1999</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>10.1186/bcr29</td>\n",
       "      <td>PMC13911</td>\n",
       "      <td>11056684.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11056684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Cancer Res</td>\n",
       "      <td>1465-5411</td>\n",
       "      <td>1465-542X</td>\n",
       "      <td>1999</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC13912</td>\n",
       "      <td>11400682.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>live</td>\n",
       "      <td>11400682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Journal Title       ISSN      eISSN  Year Volume Issue Page  \\\n",
       "0  Breast Cancer Res  1465-5411  1465-542X  2000      3     1   55   \n",
       "1  Breast Cancer Res  1465-5411  1465-542X  2000      3     1   61   \n",
       "2  Breast Cancer Res  1465-5411  1465-542X  2000      3     1   66   \n",
       "3  Breast Cancer Res  1465-5411  1465-542X  1999      2     1   59   \n",
       "4  Breast Cancer Res  1465-5411  1465-542X  1999      2     1   64   \n",
       "\n",
       "             DOI     PMCID        PMID Manuscript Id Release Date      pmid  \n",
       "0            NaN  PMC13900  11250746.0           NaN         live  11250746  \n",
       "1            NaN  PMC13901  11250747.0           NaN         live  11250747  \n",
       "2            NaN  PMC13902  11250748.0           NaN         live  11250748  \n",
       "3  10.1186/bcr29  PMC13911  11056684.0           NaN         live  11056684  \n",
       "4            NaN  PMC13912  11400682.0           NaN         live  11400682  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmid_to_pmcid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#keep the rows in the linking file that have a match from ndar; strip out letters from pmcid\n",
    "referenced_papers = pmid_to_pmcid[pmid_to_pmcid.pmid.isin(collection_pmids)]\n",
    "referenced_papers['pmcid'] = referenced_papers.PMCID.apply(lambda x: x[3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#get the file location for the matches\n",
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(referenced_papers.pmcid.unique()).sample(100, replace=False)\n",
    "#this file was manually labeled so that I can get specific passages\n",
    "sampled_papers.to_csv('output/ndar_pmcids.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_papers = pd.read_csv('output/ndar_pmcids.csv')\n",
    "#split out those that have shared and not shared data\n",
    "no_shares = sampled_papers[sampled_papers.data_sharing=='0']\n",
    "shares = sampled_papers[sampled_papers.data_sharing=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shares_locs = file_ix[file_ix.pmcid.isin(no_shares.PMCID.tolist())]\n",
    "text = []\n",
    "repo = []\n",
    "paper_offset = []\n",
    "pmcid = []\n",
    "doi = []\n",
    "section_type = []\n",
    "for i, file_path in enumerate(no_shares_locs.file):\n",
    "    with open(no_shares_locs.file.iloc[i][24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[no_shares_locs.paper_number.iloc[i]]\n",
    "        section = sample_section(paper)\n",
    "        text.append(section['text'])\n",
    "        repo.append(np.nan)\n",
    "        paper_offset.append(section['offset'])\n",
    "        pmcid.append(no_shares_locs.pmcid.iloc[i])\n",
    "        doi.append(np.nan)\n",
    "        section_type.append(section['infons']['section_type'])\n",
    "        \n",
    "dat = pd.DataFrame({'text':text,\n",
    "                    'repo':repo,\n",
    "                    'paper_offset':paper_offset,\n",
    "                    'pmcid':pmcid,\n",
    "                    'doi':doi,\n",
    "                    'section':section_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares_locs = file_ix[file_ix.pmcid.isin(shares.PMCID.tolist())]\n",
    "text = []\n",
    "repo = []\n",
    "paper_offset = []\n",
    "pmcid = []\n",
    "doi = []\n",
    "section_type = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[0][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[0]]\n",
    "    text.append(paper['documents'][0]['passages'][77]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][77]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[0])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][77]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[2][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[2]]\n",
    "    text.append(paper['documents'][0]['passages'][62]['text'])\n",
    "    repo.append('GEO')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][62]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[2])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][62]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[3][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[3]]\n",
    "    text.append(paper['documents'][0]['passages'][88]['text'])\n",
    "    repo.append('OSF')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][88]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[3])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][88]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[4][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[4]]\n",
    "    text.append(paper['documents'][0]['passages'][47]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][47]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[4])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][47]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shares_locs.file.iloc[5][24:]) as infile:\n",
    "    share_dat = json.load(infile)\n",
    "    paper = share_dat[shares_locs.paper_number.iloc[5]]\n",
    "    text.append(paper['documents'][0]['passages'][91]['text'])\n",
    "    repo.append('NDAR')\n",
    "    paper_offset.append(paper['documents'][0]['passages'][91]['offset'])\n",
    "    pmcid.append(shares_locs.pmcid.iloc[5])\n",
    "    doi.append(np.nan)\n",
    "    section_type.append(paper['documents'][0]['passages'][91]['infons']['section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2 = pd.DataFrame({'text':text,\n",
    "                    'repo':repo,\n",
    "                    'paper_offset':paper_offset,\n",
    "                    'pmcid':pmcid,\n",
    "                    'doi':doi,\n",
    "                    'section':section_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_labs = pd.concat([dat, dat2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndar_labs.to_csv('output/labeled_data/ndar_labs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do a random selection of other passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "#load file index\n",
    "file_ix = pd.read_csv('output/file_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(nimh_papers.pmcid.unique()).sample(100, replace=False)\n",
    "sampled_papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 3)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_locs = file_ix[file_ix.pmcid.isin(sampled_papers)]\n",
    "file_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dat = []\n",
    "for i, file_path in enumerate(file_locs.file):\n",
    "    with open(file_path[24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[file_locs.paper_number.iloc[i]]\n",
    "        sec = sample_section(paper)\n",
    "        out_dat.append([sec['text'], #delete the seg[0]:seg[1] and you'll have the full segment\n",
    "                np.nan, #ids which repo\n",
    "                sec['offset'], #how far into the paper?\n",
    "                file_locs.pmcid.iloc[i], \n",
    "                np.nan, \n",
    "                sec['infons']['section_type']])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(out_dat, columns=['text', 'repo', 'paper_offset', 'pmcid', 'doi', 'section'])\n",
    "temp.to_csv('output/labeled_data/random_selections.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to add some additional cases\n",
    "\n",
    "First, I'm going to pull out some of the instances identified via regex matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5726, 5)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load previously identified hits and papers funded by nimh\n",
    "hit_contexts = pd.read_csv('output/hit_contexts.csv') #need the subset that is in nimh funded papers\n",
    "nimh_papers = pd.read_csv('output/nimh_papers.csv')\n",
    "#load file index\n",
    "file_ix = pd.read_csv('output/file_index.csv')\n",
    "#filter the hits to just nimh papers\n",
    "nimh_hits = hit_contexts[hit_contexts.pmcid.isin(nimh_papers.pmcid)]\n",
    "nimh_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5463, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out the previously sampled cases\n",
    "excludes = pd.read_csv('output/labeled_data/regex_hits.csv')\n",
    "nimh_hits = nimh_hits[~nimh_hits.pmcid.isin(excludes.pmcid)]\n",
    "print(nimh_hits.shape)\n",
    "#pull a sample of 100 hits\n",
    "sampled_papers = pd.Series(nimh_hits.pmcid.unique()).sample(100, replace=False)\n",
    "sampled_papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get location of sampled hits\n",
    "file_locs = file_ix[file_ix.pmcid.isin(sampled_papers)]\n",
    "data_collect = []\n",
    "for i, file_path in enumerate(file_locs.file):\n",
    "    with open(file_path[24:]) as infile:\n",
    "        dat = json.load(infile)\n",
    "        paper = dat[file_locs.paper_number.iloc[i]]\n",
    "        out = hit_contexts(paper)\n",
    "        data_collect.extend(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_label = pd.DataFrame(data_collect, columns=['text', 'repo', 'paper_offset', 'pmcid', 'doi', 'section'])\n",
    "df_to_label = df_to_label.drop_duplicates(subset=['pmcid'])\n",
    "df_to_label.to_csv('output/labeled_data/regex_hits2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac_knowl",
   "language": "python",
   "name": "ac_knowl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
